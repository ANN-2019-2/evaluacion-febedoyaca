{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3lnWjvI83ix"
   },
   "source": [
    "# Filtado de mensajes spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recepción de publicidad no deseada a traves mensajes de texto usando SMS (Short Message Service) es un problema que afecta a muchos usuarios de teléfonos móviles. El problema radica en que los usuarios deben pagar por los mesajes recibidos, y por este motivo resulta muy importante que las compañías prestadoras del servicio puedan filtrar mensajes indeseados antes de enviarlos a su destinatario final. Los mensajes tienen una longitud máxima de 160 caracteres, por lo que el texto resulta poco para realizar la clasificación, en comparación con textos más largos (como los emails). Adicionalmente, los errores de digitación dificultan el proceso de detección automática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema en términos de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene una muestra contiene 5574 mensajes en inglés, no codificados y clasificados como legítimos (ham) o spam (http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/). La información está almacenada en el archivo `datos/spam-sms.zip`.El problema en términos de los datos consiste en clasificar si un mensaje SMS es legítico o spam, a partir del análisis de las palabras que contiente, partiendo del supuesto de que ciertas palabras que son más frecuentes dependiendo del tipo de mensaje. Esto implica que en la fase de preparación de los datos se deben extraer las palabras que contiene cada mensaje para poder realizar el análsis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximaciones posibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se desea comparar los resultados de un modelo de redes neuronales artificiales y otras técnicas estadísticas para realizar la clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usted debe:\n",
    "\n",
    "* Preprocesar los datos para representarlos usando bag-of-words.\n",
    "\n",
    "\n",
    "* Construir un modelo de regresión logística como punto base para la comparación con otros modelos más complejos.\n",
    "\n",
    "\n",
    "* Construir un modelo de redes neuronales artificiales. Asimismo, debe determinar el número de neuronas en la capa o capas ocultas.\n",
    "\n",
    "\n",
    "* Utiizar una técnica como crossvalidation u otra similar para establecer la robustez del modelo.\n",
    "\n",
    "\n",
    "* Presentar métricas de desempeño para establecer las bondades y falencias de cada clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix as matriz\n",
    "from sklearn.metrics import accuracy_score as accu\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Se leen los datos de el txt. Se crean dos columnas en el DataFrame Datos, una para ver si es ham o spam, y otra con el mensaje envidao. Luego se cambia la clasificacion de ham y spam a 1 y 0 para que luego este DataSet sirva para entrenar el modelo.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datos = pd.DataFrame (columns = ['H/S','Texto'])\n",
    "with open('datos/SMSSpamCollection.txt', encoding='utf-8') as texto:\n",
    "    Leer = texto.read().split(\"\\n\")\n",
    "    for i in Leer[:-1]:\n",
    "        Ham = \"ham\\t\"\n",
    "        Spam = \"spam\\t\"\n",
    "        CambioHam = \"ham????????????????\"\n",
    "        CambioSpam = \"spam????????????????\"\n",
    "        i = re.sub(Ham, CambioHam, i)\n",
    "        i = re.sub(Spam, CambioSpam, i)\n",
    "        i = i.split(\"????????????????\")\n",
    "        if (len(i) == 2):\n",
    "            Datos = Datos.append({'H/S': i[0], 'Texto':i[1:]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto = pd.DataFrame (columns = ['Texto'])\n",
    "for i in Datos['Texto']:\n",
    "    Texto=Texto.append({'Texto': i[0].split()}, ignore_index=True)\n",
    "    #print(i[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS = Datos['H/S'].replace({'ham': 1, 'spam': 0}).astype('int')\n",
    "stem = nltk.stem.SnowballStemmer('english', ignore_stopwords=True)\n",
    "Palabras = stopwords.words(\"english\")\n",
    "\n",
    "PalabrasFinal = []\n",
    "for i in Texto['Texto']:\n",
    "    ST = [stem.stem(a) for a in i]\n",
    "    PalabrasFinal.append(' '.join(ST))\n",
    "\n",
    "PalabrasFinal = list(filter(None, PalabrasFinal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Se crea el Bag of words que se va a utilizar para probar el modelo.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vec = CountVectorizer(PalabrasFinal)\n",
    "VexTex = Vec.fit_transform(PalabrasFinal)\n",
    "VexTex= VexTex.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Entrenamiento y regresi&oacute;n lineal sobre los datos.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "XEnt, XTest, YEnt, YTest = train_test_split(VexTex, HS, stratify=HS, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(XEnt,YEnt)\n",
    "Pred = LR.predict(XTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Matriz de confusi&oacute;n de la predicci&oacute;n de la regresi&oacute;n lineal.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126  23]\n",
      " [  2 964]]\n"
     ]
    }
   ],
   "source": [
    "print(matriz(YTest,Pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Accuracy de la predicci&oacute;n de la regresi&oacute;n lineal.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775784753363229\n"
     ]
    }
   ],
   "source": [
    "print(accu(YTest,Pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style='color: rgb(0, 0, 0); font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Peque&ntilde;a prueba para ver si esta corriendo el tensorflow o no, en versiones anteriores no esta corriendo por defecto.</span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.executing_eagerly() == False:\n",
    "    tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: \n",
    "      20px;\">Creaci&oacute;n de modelo de redes neuronales</span></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>B&uacute;squeda de numero optimo de neuronas por capa.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 0s 401us/sample - loss: 0.0431 - acc: 0.9901 - mse: 0.0085\n",
      "1115/1115 [==============================] - 1s 478us/sample - loss: 0.0458 - acc: 0.9883 - mse: 0.0096\n",
      "1115/1115 [==============================] - 1s 476us/sample - loss: 0.0810 - acc: 0.9821 - mse: 0.0142\n",
      "1115/1115 [==============================] - 0s 398us/sample - loss: 0.0875 - acc: 0.9821 - mse: 0.0151\n",
      "1115/1115 [==============================] - 1s 489us/sample - loss: 0.0905 - acc: 0.9821 - mse: 0.0152\n",
      "1115/1115 [==============================] - 1s 458us/sample - loss: 0.0896 - acc: 0.9830 - mse: 0.0148\n",
      "1115/1115 [==============================] - 0s 425us/sample - loss: 0.0905 - acc: 0.9821 - mse: 0.0149\n",
      "1115/1115 [==============================] - 0s 406us/sample - loss: 0.0986 - acc: 0.9821 - mse: 0.0152\n",
      "1115/1115 [==============================] - 0s 429us/sample - loss: 0.0968 - acc: 0.9839 - mse: 0.0148\n"
     ]
    }
   ],
   "source": [
    "ResultadosN = [0,0]\n",
    "for i in range(1,10):\n",
    "    Modelo = keras.Sequential()\n",
    "\n",
    "    Modelo.add(keras.layers.Dense(units = i, activation=tf.nn.relu, input_dim = XEnt.shape[1]))\n",
    "    Modelo.add(keras.layers.Dense(units = 1, activation=tf.nn.sigmoid))\n",
    "    \n",
    "    Modelo.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc','mse'])\n",
    "    \n",
    "    Modelo.fit(XEnt,YEnt,epochs=50,batch_size=30, verbose=0)\n",
    "    a = Modelo.evaluate(XTest, YTest)\n",
    "    if(a[1]>ResultadosN[1]):\n",
    "        ResultadosN = [i,a[1]]\n",
    "    Modelo = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>B&uacute;squeda de numero optimo de capas.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 0s 448us/sample - loss: 0.0432 - acc: 0.9901 - mse: 0.0086\n",
      "1115/1115 [==============================] - 1s 528us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n",
      "1115/1115 [==============================] - 1s 483us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n",
      "1115/1115 [==============================] - 1s 467us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n",
      "1115/1115 [==============================] - 1s 530us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n",
      "1115/1115 [==============================] - 1s 513us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n",
      "1115/1115 [==============================] - 1s 595us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n",
      "1115/1115 [==============================] - 1s 608us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n",
      "1115/1115 [==============================] - 1s 520us/sample - loss: 0.3932 - acc: 0.8664 - mse: 0.1158\n"
     ]
    }
   ],
   "source": [
    "ResultadosC = [0,0]\n",
    "for i in range(1,10): \n",
    "    Modelo = keras.Sequential()\n",
    "    \n",
    "    for j in range (0,i):\n",
    "        Modelo.add(keras.layers.Dense(units = ResultadosN[0], activation=tf.nn.relu, input_dim = XEnt.shape[1]))\n",
    "        \n",
    "    Modelo.add(keras.layers.Dense(units = 1, activation=tf.nn.sigmoid))\n",
    "    \n",
    "    Modelo.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc','mse'])\n",
    "    \n",
    "    Modelo.fit(XEnt,YEnt,epochs=50,batch_size=30, verbose=0)\n",
    "    a = Modelo.evaluate(XTest, YTest)\n",
    "    if(a[1]>ResultadosC[1]):\n",
    "        ResultadosC = [i,a[1]]\n",
    "    Modelo = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.99013454] [1, 0.99013454]\n"
     ]
    }
   ],
   "source": [
    "print(ResultadosN, ResultadosC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Modelo con numero de capas y neuronas optimo.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 0s 415us/sample - loss: 0.0465 - acc: 0.9874 - mse: 0.0110\n"
     ]
    }
   ],
   "source": [
    "Modelo = keras.Sequential()\n",
    "\n",
    "for j in range(0,ResultadosC[0]):\n",
    "    Modelo.add(keras.layers.Dense(units =ResultadosN[0], activation=tf.nn.relu, input_dim = XEnt.shape[1]))\n",
    "    \n",
    "Modelo.add(keras.layers.Dense(units = 1, activation=tf.nn.sigmoid))\n",
    "    \n",
    "Modelo.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc','mse'])\n",
    "\n",
    "Modelo.fit(XEnt,YEnt,epochs=100,batch_size=30,verbose=0)\n",
    "a = Modelo.evaluate(XTest, YTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Prueba con Cross Validation para el modelo &oacute;ptimo encontrado antes.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 1s 514us/sample - loss: 0.1090 - acc: 0.9830 - mse: 0.0145\n",
      "1115/1115 [==============================] - 0s 422us/sample - loss: 0.1316 - acc: 0.9758 - mse: 0.0194\n",
      "1115/1115 [==============================] - 0s 411us/sample - loss: 0.1336 - acc: 0.9812 - mse: 0.0183\n",
      "1114/1114 [==============================] - 0s 424us/sample - loss: 0.1940 - acc: 0.9713 - mse: 0.0239\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'perdictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f69758b7f2c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mIndiceEnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndiceTest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXEnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndiceEnt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIndiceTest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXEnt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYEnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperdictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'perdictions' is not defined"
     ]
    }
   ],
   "source": [
    "def validacion(Indice, IndiceTest, Variables, Resultados):\n",
    "    XEnt = Variables[Indice]\n",
    "    XTest = Variables[IndiceTest]\n",
    "    YEnt = Resultados.iloc[Indice]\n",
    "    YTest = Resultados.iloc[IndiceTest]\n",
    "\n",
    "    Modelo = keras.Sequential()\n",
    "\n",
    "    for j in range(0,ResultadosC[0]):\n",
    "        Modelo.add(keras.layers.Dense(units =ResultadosN[0], activation=tf.nn.relu, input_dim = XEnt.shape[1]))\n",
    "\n",
    "    Modelo.add(keras.layers.Dense(units = 1, activation=tf.nn.sigmoid))\n",
    "\n",
    "    Modelo.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc','mse'])\n",
    "\n",
    "    Modelo.fit(XEnt,YEnt,epochs=100,batch_size=30,verbose=0)\n",
    "    a = Modelo.evaluate(XTest, YTest)\n",
    "    \n",
    "    return a\n",
    "\n",
    "K = KFold(n_splits=4)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for IndiceEnt, IndiceTest in K.split(XEnt):\n",
    "    predictions.append(validacion(IndiceEnt,IndiceTest,XEnt,YEnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019033532589673996\n",
      "0.9777962565422058\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "c = []\n",
    "for i in predictions:\n",
    "    b.append(i[2])\n",
    "    c.append(i[1])\n",
    "print(sum(b)/len(b))\n",
    "print(sum(c)/len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: \n",
    "      22px;\"><strong>Conclusiones</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: \n",
    "      18px;\">Accuracy</span></strong><br><br>- Regresi&oacute;n lineal: 0.9775784753363229</p>\n",
    "<p>- Red neuronal: 0.9777962565422058</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Copia de Untitled3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
